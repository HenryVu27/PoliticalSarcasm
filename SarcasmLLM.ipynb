{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sarcasm Detection using Pretrained Language Models\n",
        "\n",
        "This notebook implements sarcasm detection on Reddit political comments using transformer-based pretrained language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and set up paths\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "base_dir = \"/content/drive/MyDrive/SarcasmDetection\"\n",
        "sys.path.append(base_dir)\n",
        "os.chdir(base_dir)\n",
        "\n",
        "# Data paths\n",
        "train_data_balanced_path = os.path.join(base_dir, \"train-balanced-sarcasm.csv\")\n",
        "test_data_balanced_path = os.path.join(base_dir, \"test-balanced.csv\")\n",
        "test_data_unbalanced_path = os.path.join(base_dir, \"test-unbalanced.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets evaluate scikit-learn pandas numpy matplotlib seaborn torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv(train_data_balanced_path)\n",
        "test_data_balanced = pd.read_csv(test_data_balanced_path)\n",
        "test_data_unbalanced = pd.read_csv(test_data_unbalanced_path)\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data (balanced) shape: {test_data_balanced.shape}\")\n",
        "print(f\"Test data (unbalanced) shape: {test_data_unbalanced.shape}\")\n",
        "\n",
        "# Display sample data\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "# Class distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='label', data=train_data)\n",
        "plt.title('Distribution of Sarcastic vs Non-Sarcastic Comments')\n",
        "plt.xlabel('Sarcasm (1=Yes, 0=No)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Check comment length distribution\n",
        "train_data['comment_length'] = train_data['comment'].fillna('').apply(len)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=train_data, x='comment_length', hue='label', bins=50, kde=True)\n",
        "plt.title('Comment Length Distribution')\n",
        "plt.xlabel('Comment Length (characters)')\n",
        "plt.ylabel('Count')\n",
        "plt.xlim(0, 1000)  # Limiting x-axis for better visualization\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill missing comments with empty string\n",
        "train_data['comment'] = train_data['comment'].fillna('')\n",
        "test_data_balanced['comment'] = test_data_balanced['comment'].fillna('')\n",
        "\n",
        "# Basic preprocessing - remove deleted comments\n",
        "train_data = train_data[~train_data['comment'].str.contains('[deleted]|[removed]', case=False, regex=True)]\n",
        "test_data_balanced = test_data_balanced[~test_data_balanced['comment'].str.contains('[deleted]|[removed]', case=False, regex=True)]\n",
        "\n",
        "# Check distribution after preprocessing\n",
        "print(f\"Training data shape after preprocessing: {train_data.shape}\")\n",
        "print(f\"Test data shape after preprocessing: {test_data_balanced.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Context-Aware Preprocessing\n",
        "\n",
        "We enhance the model's understanding by including parent comments as context when available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add context from parent comments where available\n",
        "def prepare_context_aware_input(row):\n",
        "    if pd.notna(row['parent_comment']) and row['parent_comment'] != '':\n",
        "        # Truncate parent comment if too long\n",
        "        parent = row['parent_comment'][:500] + '...' if len(row['parent_comment']) > 500 else row['parent_comment']\n",
        "        return f\"Parent: {parent} Comment: {row['comment']}\"\n",
        "    else:\n",
        "        return f\"Comment: {row['comment']}\"\n",
        "\n",
        "# Apply to create context-aware inputs\n",
        "train_data['context_input'] = train_data.apply(prepare_context_aware_input, axis=1)\n",
        "test_data_balanced['context_input'] = test_data_balanced.apply(prepare_context_aware_input, axis=1)\n",
        "\n",
        "# Display a few examples\n",
        "print(\"Context-aware inputs:\")\n",
        "for i in range(3):\n",
        "    print(f\"Example {i+1}: {train_data['context_input'].iloc[i][:200]}...\")\n",
        "    print(f\"Label: {'Sarcastic' if train_data['label'].iloc[i] == 1 else 'Not sarcastic'}\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformer Model Implementation\n",
        "\n",
        "We'll use the Hugging Face Transformers library to fine-tune a pretrained model for sarcasm detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "\n",
        "# Define model name - you can change this to other models\n",
        "model_name = \"roberta-base\"  # Options: \"distilbert-base-uncased\", \"albert-base-v2\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, \n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Print model info\n",
        "print(f\"Using model: {model_name}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "# Use a smaller subset for faster training in this notebook\n",
        "sample_size = 100000  # Adjust based on your computational resources\n",
        "if len(train_data) > sample_size:\n",
        "    train_data_sample = train_data.sample(sample_size, random_state=42)\n",
        "    print(f\"Using {sample_size} samples from training data\")\n",
        "else:\n",
        "    train_data_sample = train_data\n",
        "    print(f\"Using all {len(train_data)} training samples\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_data_sample['context_input'].tolist(),\n",
        "    train_data_sample['label'].tolist(),\n",
        "    test_size=0.1,\n",
        "    stratify=train_data_sample['label'],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize data\n",
        "# We use a max length of 256 tokens - adjust based on your dataset\n",
        "max_length = 256\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings = tokenizer(test_data_balanced['context_input'].tolist(), \n",
        "                         truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "# Create torch datasets\n",
        "class SarcasmDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SarcasmDataset(train_encodings, train_labels)\n",
        "val_dataset = SarcasmDataset(val_encodings, val_labels)\n",
        "test_dataset = SarcasmDataset(test_encodings, test_data_balanced['label'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class weights to handle imbalanced data\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(f\"Class weights: {class_weights_dict}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,  # Adjust as needed\n",
        "    per_device_train_batch_size=16,  # Adjust based on your GPU memory\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    label_smoothing_factor=0.1,  # Helps with overconfidence\n",
        ")\n",
        "\n",
        "# Define metrics for evaluation\n",
        "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "val_results = trainer.evaluate()\n",
        "print(f\"Validation results: {val_results}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f\"Test results: {test_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Detailed Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get detailed predictions on test set\n",
        "test_pred_output = trainer.predict(test_dataset)\n",
        "test_preds = np.argmax(test_pred_output.predictions, axis=1)\n",
        "\n",
        "# Create classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_data_balanced['label'], test_preds))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_data_balanced['label'], test_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - Transformer Model')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Error Analysis\n",
        "\n",
        "Let's look at some examples the model got wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame with predictions\n",
        "test_df = test_data_balanced.copy()\n",
        "test_df['predicted'] = test_preds\n",
        "test_df['correct'] = test_df['label'] == test_df['predicted']\n",
        "\n",
        "# False positives (predicted sarcastic but not)\n",
        "false_positives = test_df[(test_df['predicted'] == 1) & (test_df['label'] == 0)]\n",
        "print(f\"Number of false positives: {len(false_positives)}\")\n",
        "print(\"\\nExamples of false positives (predicted sarcastic but not):\")\n",
        "for i in range(min(5, len(false_positives))):\n",
        "    print(f\"Example {i+1}: {false_positives['comment'].iloc[i][:200]}...\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# False negatives (predicted not sarcastic but is)\n",
        "false_negatives = test_df[(test_df['predicted'] == 0) & (test_df['label'] == 1)]\n",
        "print(f\"\\nNumber of false negatives: {len(false_negatives)}\")\n",
        "print(\"\\nExamples of false negatives (predicted not sarcastic but is):\")\n",
        "for i in range(min(5, len(false_negatives))):\n",
        "    print(f\"Example {i+1}: {false_negatives['comment'].iloc[i][:200]}...\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interpreting Model Predictions\n",
        "\n",
        "Here we'll analyze a few examples to see what the model focuses on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a classifier from our fine-tuned model\n",
        "classifier = pipeline(\n",
        "    \"text-classification\", \n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        ")\n",
        "\n",
        "# Function to perform word importance analysis\n",
        "def analyze_word_importance(text, classifier, top_n=10):\n",
        "    # Get base prediction\n",
        "    base_pred = classifier(text)[0]\n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    word_importance = []\n",
        "    \n",
        "    for i in range(len(words)):\n",
        "        # Skip if the word is too short\n",
        "        if len(words[i]) <= 2:\n",
        "            continue\n",
        "            \n",
        "        # Create a version with this word masked\n",
        "        masked_words = words.copy()\n",
        "        masked_words[i] = '[MASK]'\n",
        "        masked_text = ' '.join(masked_words)\n",
        "        \n",
        "        # Check prediction change\n",
        "        masked_result = classifier(masked_text)[0]\n",
        "        importance = abs(base_pred['score'] - masked_result['score'])\n",
        "        word_importance.append((words[i], importance))\n",
        "    \n",
        "    # Sort by importance\n",
        "    word_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_importance[:top_n]\n",
        "\n",
        "# Choose some examples to analyze\n",
        "examples = [\n",
        "    test_df[test_df['label'] == 1]['context_input'].iloc[0],  # Sarcastic \n",
        "    test_df[test_df['label'] == 0]['context_input'].iloc[0],  # Non-sarcastic\n",
        "    false_positives['context_input'].iloc[0],  # False positive\n",
        "    false_negatives['context_input'].iloc[0]   # False negative\n",
        "]\n",
        "\n",
        "# Analyze each example\n",
        "for i, example in enumerate(examples):\n",
        "    print(f\"Example {i+1}: {example[:200]}...\")\n",
        "    result = classifier(example)[0]\n",
        "    print(f\"Prediction: {result['label']}, Confidence: {result['score']:.4f}\")\n",
        "    \n",
        "    important_words = analyze_word_importance(example, classifier)\n",
        "    print(\"\\nMost important words:\")\n",
        "    for word, score in important_words:\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Subreddit Analysis with Transformer Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add predictions to train dataset (using a subsample for efficiency)\n",
        "# This requires some modifications since we might not have predictions for all data\n",
        "sample_train_texts = train_data.sample(5000, random_state=42)['context_input'].tolist()\n",
        "sample_train_encodings = tokenizer(sample_train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "sample_train_dataset = SarcasmDataset(\n",
        "    sample_train_encodings, \n",
        "    [0] * len(sample_train_texts)  # Dummy labels, we only need predictions\n",
        ")\n",
        "\n",
        "# Get predictions\n",
        "sample_pred_output = trainer.predict(sample_train_dataset)\n",
        "sample_probs = sample_pred_output.predictions\n",
        "sample_preds = np.argmax(sample_probs, axis=1)\n",
        "\n",
        "# Get the confidence scores for the positive class (sarcasm)\n",
        "sample_sarcasm_probs = sample_probs[:, 1]\n",
        "\n",
        "# Create a DataFrame with indices\n",
        "train_sample_idx = train_data.sample(5000, random_state=42).index\n",
        "pred_df = pd.DataFrame({\n",
        "    'index': train_sample_idx,\n",
        "    'predicted': sample_preds,\n",
        "    'sarcasm_confidence': sample_sarcasm_probs\n",
        "})\n",
        "\n",
        "# Merge with original data\n",
        "train_with_preds = train_data.loc[pred_df['index']].copy()\n",
        "train_with_preds['predicted'] = pred_df['predicted'].values\n",
        "train_with_preds['sarcasm_confidence'] = pred_df['sarcasm_confidence'].values\n",
        "\n",
        "# Analyze by subreddit\n",
        "subreddit_analysis = train_with_preds.groupby('subreddit').agg({\n",
        "    'label': 'mean',  # True sarcasm rate\n",
        "    'predicted': 'mean',  # Predicted sarcasm rate\n",
        "    'sarcasm_confidence': 'mean',  # Average confidence\n",
        "    'index': 'count'  # Count of comments\n",
        "}).rename(columns={'index': 'count', 'label': 'true_sarcasm_rate', 'predicted': 'predicted_sarcasm_rate'})\n",
        "\n",
        "# Filter for subreddits with enough comments\n",
        "min_comments = 20\n",
        "subreddit_analysis = subreddit_analysis[subreddit_analysis['count'] >= min_comments]\n",
        "\n",
        "# Sort by true sarcasm rate\n",
        "subreddit_analysis = subreddit_analysis.sort_values('true_sarcasm_rate', ascending=False)\n",
        "\n",
        "# Display top 20 subreddits\n",
        "print(\"Top 20 subreddits by true sarcasm rate:\")\n",
        "subreddit_analysis.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot subreddit analysis\n",
        "top_n = 15\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Get top N subreddits by sarcasm rate with at least min_comments\n",
        "top_subreddits = subreddit_analysis.head(top_n)\n",
        "\n",
        "# Plot true vs predicted sarcasm rates\n",
        "top_subreddits[['true_sarcasm_rate', 'predicted_sarcasm_rate']].plot(\n",
        "    kind='bar', \n",
        "    color=['firebrick', 'steelblue']\n",
        ")\n",
        "\n",
        "plt.title(f'Top {top_n} Subreddits by Sarcasm Rate (min {min_comments} comments)')\n",
        "plt.ylabel('Sarcasm Rate')\n",
        "plt.xlabel('Subreddit')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(['True Sarcasm Rate', 'Predicted Sarcasm Rate'])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Creating a Practical Sarcasm Detection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sarcasm(text, context=None, classifier=None):\n",
        "    \"\"\"Predict whether text is sarcastic\"\"\"\n",
        "    if classifier is None:\n",
        "        # Load model if not provided - this assumes the model is already trained\n",
        "        classifier = pipeline(\n",
        "            \"text-classification\", \n",
        "            model=model,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "    \n",
        "    # Prepare input with context if available\n",
        "    if context:\n",
        "        input_text = f\"Parent: {context} Comment: {text}\"\n",
        "    else:\n",
        "        input_text = f\"Comment: {text}\"\n",
        "    \n",
        "    # Get prediction\n",
        "    result = classifier(input_text)[0]\n",
        "    is_sarcastic = result['label'] == 'LABEL_1'\n",
        "    \n",
        "    # Return formatted result\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"is_sarcastic\": is_sarcastic,\n",
        "        \"confidence\": result['score'],\n",
        "        \"context\": context\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "examples = [\n",
        "    {\"text\": \"Yeah, sure, that's definitely going to work out great.\", \n",
        "     \"context\": \"We should cut taxes for the rich to stimulate the economy.\"},\n",
        "    {\"text\": \"This new policy will significantly reduce our carbon footprint.\", \n",
        "     \"context\": \"The government announced new climate regulations today.\"}\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    result = predict_sarcasm(example[\"text\"], example[\"context\"], classifier)\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Is sarcastic: {result['is_sarcastic']} (Confidence: {result['confidence']:.4f})\")\n",
        "    print(f\"Context: {result['context']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Comparing with Classical ML Models\n",
        "\n",
        "If you have results from the previous notebook with classical models, you can compare them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your baseline model results here\n",
        "# This is a placeholder - fill in with your actual results from the previous notebook\n",
        "baseline_results = {\n",
        "    'Bag of Words': {'accuracy': 0.70, 'f1': 0.69},\n",
        "    'TF-IDF': {'accuracy': 0.72, 'f1': 0.71},\n",
        "    'Combined': {'accuracy': 0.74, 'f1': 0.73}\n",
        "}\n",
        "\n",
        "# Add transformer results\n",
        "transformer_accuracy = test_results['eval_accuracy']\n",
        "transformer_f1 = test_results['eval_f1']\n",
        "baseline_results['Transformer'] = {'accuracy': transformer_accuracy, 'f1': transformer_f1}\n",
        "\n",
        "# Plot comparison\n",
        "models = list(baseline_results.keys())\n",
        "accuracies = [baseline_results[model]['accuracy'] for model in models]\n",
        "f1_scores = [baseline_results[model]['f1'] for model in models]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.bar(x - width/2, accuracies, width, label='Accuracy', color='darkblue')\n",
        "ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='darkred')\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
        "    \n",
        "for i, v in enumerate(f1_scores):\n",
        "    ax.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save the Model for Future Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "save_path = os.path.join(base_dir, \"transformer_sarcasm_model\")\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"Model and tokenizer saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to use a transformer-based pretrained language model to detect sarcasm in political comments. The key advantages of this approach include:\n",
        "\n",
        "1. **Context awareness**: By incorporating parent comments as context, the model can better understand subtle sarcasm cues.\n",
        "2. **Transfer learning**: Using pretrained models provides strong language understanding capabilities out of the box.\n",
        "3. **Performance**: Transformer models typically outperform classical ML approaches for this type of task.\n",
        "\n",
        "Practical applications include:\n",
        "- Social media moderation to identify potentially misinterpreted content\n",
        "- Political sentiment analysis that can distinguish between genuine and sarcastic opinions\n",
        "- Research tools to analyze communication patterns in political discourse"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
